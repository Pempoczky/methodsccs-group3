library(ggplot2)
ggplot(diamonds, aes(x=log(price)))+geom_histogram()+stat_function(fun=dnorm, args=list(mean = mean(log(diamonds$price)), sd = sd(log(diamonds$price))))
source("~/Year2/Block1/Statistics/rfile.r", echo=TRUE)
source("~/Year2/Block1/Statistics/rfile.r", echo=TRUE)
source("~/Year2/Block1/Statistics/rfile.r", echo=TRUE)
?dbinom
?rbinom
library(reshape2)
sleep
library(datasets)
sleep
?sleep
boxplot(extra~group,data=sleep, main="Sleep Data",
xlab="Group", ylab="Extra")
ggplot(sleep, aes(x = group, y = extra)) + geom boxplot()
ggplot(diamonds, aes(x = color, y = price)) + geom boxplot()
library(ggplot2)
ggplot(sleep, aes(x = group, y = extra)) + geom boxplot()
ggplot(sleep, aes(x = group, y = extra)) + geom_boxplot()
ggplot(sleep, aes(x = group, y = extra)) + geom_point()
?sleep
?t.test
with(sleep, t.test(extra[group == 1], extra[group == 2], conf.level=0.95))
with(sleep, t.test(extra[group == 1]- extra[group == 2], conf.level=0.95))
load("~/.RData")
data <- read.table(choose.files())
sleep
boxplot(sleep$extra)
scatterplot(sleep)
scatterplot(sleep$extra)
plot(sleep$extra, sleep$ID)
?prop.test
heads <- rbinom(1, size = 100, prob = .5)
prop.test(heads, 100, conf.level = 0.95)
?t.test
t.test(1:10, y = c(7:20))
prop.test(heads, 100, conf.level = 0.95)
?xtabs
ToothGrowth
summary(lm(len ~ supp + dose, ToothGrowth))
summary(lm(len ~ factor(supp) + factor(dose), ToothGrowth))
abline(lm(len ~ factor(supp) + factor(dose), ToothGrowth))
abline(lm(len ~ supp + dose, ToothGrowth))
abline(lm(len ~ supp, ToothGrowth))
plot(lm(len ~ factor(supp) + factor(dose), ToothGrowth))
summary(stepAIC(lm(len ~ factor(supp) + factor(dose), ToothGrowth)))
?stepAIC
library(reshape2)
library(ggplot2)
?stepAIC
summary(AIC(lm(len ~ factor(supp) + factor(dose), ToothGrowth)))
AIC(lm(len ~ factor(supp) + factor(dose), ToothGrowth))
TukeyHSD(aov(len ~ factor(dose), data=ToothGrowth))
interaction.plot(supp$ToothGrowth, factor(dose$ToothGrowth), len$ToothGrowth)
ToothGrowth
interaction.plot(factor(supp$ToothGrowth), factor(dose$ToothGrowth), len$ToothGrowth)
ToothGrowth$supp
interaction.plot(ToothGrowth$supp, factor(ToothGrowth$dose), ToothGrowth$len)
shoes
??stepAIC
library(MASS)
?stepAIC
summary(stepAIC(lm(len ~ factor(supp) + factor(dose), ToothGrowth)))
stepAIC(lm(len ~ factor(supp) + factor(dose), ToothGrowth))
homedata
?l
>I
?I
selectedMovies
selectMethod()
pizza
?runif
runif(10)
runif(10)<0.5
stepAIC(lm(len ~ factor(supp) + factor(dose), ToothGrowth))
stepAIC(lm(len ~ factor(supp) * factor(dose), ToothGrowth))
data <- read.table(choose.files())
hist(data$salary)
hist(data$salary[statistics==TRUE])
hist(data$salary[data$statistics==TRUE])
hist(data$salary[data$statistics==FALSE])
data
with(data, wilcox.test(salary ~ statistics, alternative="less"))
with(data, wilcox.test(salary ~ statistics, alternative="greater"))
?wilcox.test
grading <- read.table(choose.files())
supermarket <- read.table(choose.files())
fortunewheel <- read.table(choose.files())
grading
grading$time[coffee=="1"]
grading$time[grading$coffee=="1"]
t.test(grading$time[grading$coffee=="0"], grading$time[grading$coffee=="1"], alternative="less")
t.test(grading$time[grading$coffee=="0"], grading$time[grading$coffee=="1"], alternative="greater")
summary(lm(time ~ factor(coffee), grading))
plot(lm(time ~ factor(coffee), grading))
hist(time$grading)
hist(grading$time)
t.test(grading$time[grading$coffee=="0"], grading$time[grading$coffee=="2"], alternative="greater")
t.test(grading$time[grading$coffee=="0"], grading$time[grading$coffee=="2"], alternative="less")
t.test(grading$time[grading$coffee=="1"], grading$time[grading$coffee=="2"], alternative="less")
t.test(grading$time[grading$coffee=="1"], grading$time[grading$coffee=="2"], alternative="greater")
t.test()
?t.test()
summary(aov(time ~ factor(coffee), grading))
t.test(grading$time[grading$coffee=="1"], grading$time[grading$coffee=="0"], alternative="less")
t.test(grading$time[grading$coffee=="2"], grading$time[grading$coffee=="0"], alternative="less")
summary(lm(time ~ coffee, grading))
summary(lm(time ~ factor(coffee), grading))
summary(aov(lm(time ~ factor(coffee), grading)))
summary(anova(lm(time ~ factor(coffee), grading)))
anova(lm(time ~ factor(coffee), grading))
summary(lm(time ~ factor(coffee) * factor(day), grading))
summary(lm(time ~ factor(coffee) + factor(day), grading))
summary(lm(time ~ factor(coffee) + day, grading))
summary(lm(time ~ factor(coffee) + factor(day), grading))
summary(lm(time ~ factor(coffee) * factor(day), grading))
summary(lm(time ~ factor(day) * factor(coffee), grading))
summary(aov(lm(time ~ factor(day) * factor(coffee), grading)))
plot(lm(time ~ factor(day) * factor(coffee), grading))
plot(lm(time ~ factor(day) * factor(coffee), grading))
AIC(lm(time ~ factor(coffee) * factor(day), grading))
BIC(lm(time ~ factor(coffee) * factor(day), grading))
AIC(lm(time ~ factor(coffee), grading))
BIC(lm(time ~ factor(coffee), grading))
summary((lm(time ~ factor(day) * factor(coffee), grading))
summary(lm(time ~ factor(day) * factor(coffee), grading))
summary(lm(time ~ factor(coffee), grading))
supermarket
summary(lm(spending ~ time + factor(cart)))
summary(lm(spending ~ time + factor(cart), supermarket))
plot(lm(spending ~ time + factor(cart), supermarket))
plot(lm(spending ~ time + cart, supermarket))
plot(lm(spending ~ time * factor(cart), supermarket))
plot(lm(spending ~ time * cart, supermarket))
t.test(spending$supermarket[cart$supermarket=="Yes"], spending$supermarket[cart$supermarket=="No"], alternative="greater")
t.test(supermarket$spending[supermarket$cart=="Yes"], supermarket$spending[supermarket$cart=="No"], alternative="greater")
plot(lm(spending ~ factor(cart) * time, supermarket))
fortunewheel
table(fortunewheel$outcome)
probabilities <- c(0.15, 0.3, 0.04, 0.01, 0.5)
chisq.test(table(fortunewheel$outcome), probabilities)
chisq.test(table(fortunewheel$outcome), probabilities, simulate.p.value = TRUE)
chisq.test(table(fortunewheel$outcome))
chisq.test(table(fortunewheel$outcome), rep(1/5, 5))
chisq.test(table(fortunewheel$outcome), c(rep(1/5, 5))
chisq.test(table(fortunewheel$outcome), c(rep(1/5, 5)))
chisq.test(table(fortunewheel$outcome), c(rep(5, 1/5)))
chisq.test(table(fortunewheel$outcome), rep(5, 1/5))
rep(5, 1.5)
rep(5, 1/5)
replicate(5, 1/5)
chisq.test(table(fortunewheel$outcome), replicate(5, 1/5))
chisq.test(table(fortunewheel$outcome), c(replicate(5, 1/5))
chisq.test(table(fortunewheel$outcome), c(replicate(5, 1/5)))
c(replicate(5, 1/5))
chisq.test(table(fortunewheel$outcome), (0.2,0.2,0.2,0.2,0.2))
chisq.test(table(fortunewheel$outcome), c(0.2,0.2,0.2,0.2,0.2))
chisq.test(c(6, 5, 3, 1, 9), c(0.2, 0.2, 0.2, 0.2, 0.2))
chisq.test(table(fortunewheel$outcome), probabilities)
equalprobs <- c(0.2, 0.2, 0.2, 0.2, 0.2)
chisq.test(table(fortunewheel$outcome), equalprobs)
fortunewheel
table(fortunewheel$outcome)
table(fortunewheel$previous)
table(outcome, previous)
table(fortunewheel$outcome, fortunewheel$previous)
plot(lm(log(spending) ~ time + factor(cart), supermarket))
hist(residuals(lm(log(spending) ~ time * cart, supermarket)))
plot(lm(log(spending) ~ time * factor(cart), supermarket))
summary(lm(log(spending) ~ time * factor(cart), supermarket))
summary(lm(log(spending) ~ time * cart, supermarket))
chisq.test(table(fortunewheel$outcome), p=probabilities)
chisq.test(table(fortunewheel$outcome), probabilities)
busdata <- read.table(choose.files())
learning <- read.table(choose.files())
examplanning <- read.table(choose.files())
learning
factor(learning$method)
traditional <- learning$correct[learning$method == 1]
nontraditional <- learning$correct[learning$method == 2] + learning$correct[learning$method == 3]
nontraditional
learning$correct
learning$correct[index==1]
learning$correct[learning$index==1]
learning$correct[learning$method==1]
traditional
?sentence
?jon
?join
?add
table(learning$correct, learning$method)
table(learning$method, learning$correct)
chisq.test(table(learning$method, learning$correct))
learning
table(learning$method[learning$index < 5], learning$correct[learning$index > 16])
table(learning$method[learning$index < 5], learning$correct[learning$index > 5])
table(learning$method[learning$index < 5], learning$correct[learning$index < 5])
table(learning$method[learning$index > 16], learning$correct[learning$index > 16])
table(learning$correct[learning$index > 16])
table(learning$correct[learning$index < 5])
chisq.test(table(learning$correct[learning$index > 16]), table(learning$correct[learning$index < 5]))
chisq.test(c(40, 80), c(32, 88))
?prop.test()
40+80
32+88
prop.test(c(40, 80), c(32, 88))
prop.test(c(40, 32), n=120)
table(40, 32)
table(learning$correct[learning$index < 5], learning$correct[learning$index > 16])
chisq.test(table(learning$correct[learning$index < 5], learning$correct[learning$index > 16]))
prop.test(table(learning$correct[learning$index < 5], learning$correct[learning$index > 16]))
prop.test(c(40, 32), n=c(120, 120)
prop.test(c(40, 32), n=c(120, 120))
examplanning
shapiro.test(examplanning$grade)
summary(lm(grade ~ factor(day) * factor(time))
summary(lm(grade ~ factor(day) * factor(time)))
summary(lm(grade ~ factor(day) * factor(time), examplanning))
summary(lm(grade ~ day * time, examplanning))
summary(aov(lm(grade ~ day * time, examplanning)))
TukeyHSD(aov(lm(grade ~ day * time, examplanning)))
aggregate(examplanning$grade ~ examplanning$day + examplanning$time, FUN=mean)
aggregate(examplanning$grade ~ examplanning$day * examplanning$time, FUN=mean)
summary(lm(grade ~ day * time, examplanning))
TukeyHSD(aov(lm(grade ~ day * time, examplanning)))
summary(glm(ontime ~ day * holiday * rain * windspeed * temperature))
summary(glm(ontime ~ day * holiday * rain * windspeed * temperature, busdata))
summary(glm(ontime ~ day * holiday * rain * windspeed * temperature, busdata, family=binomial))
summary(glm(ontime ~ day + holiday + rain + windspeed + temperature, busdata, family=binomial))
summary(glm(ontime ~ factor(day) + holiday + rain + windspeed + temperature, busdata, family=binomial))
head(busdata)
summary(glm(ontime ~ factor(day) + factor(holiday) + rain + windspeed + temperature, busdata, family=binomial))
stepAIC(glm(ontime ~ factor(day) + factor(holiday) + rain + windspeed + temperature, busdata, family=binomial))
library(MASS)
stepAIC(glm(ontime ~ factor(day) + factor(holiday) + rain + windspeed + temperature, busdata, family=binomial))
?aggregate
grading
load("~/Year4/Bachelor's project/data/filtered_fixations.csv")
load("~/Year4/Bachelor's project/data/filtered_trials.csv")
source("~/Year4/Bachelor's project/data/filtered_trials.csv")
source("/Year4/Bachelor's project/data/filtered_trials.csv")
source("~/Year4/Bachelor's project/data/filtered_trials.csv")
read.csv("~/Year4/Bachelor's project/data/filtered_trials.csv")
data = read.csv("~/Year4/Bachelor's project/data/filtered_trials.csv")
head(data)
data[0]
data[0][1]
lm(formula = time_to_right ~ condition, data = data)
condition_model = lm(formula = time_to_right ~ condition, data = data)
plot(condition_model)
plot(lm(formula = time_to_right ~ condition, data = subset(data, participant == 1)))
hist(residuals(condition_model))
qqnorm(residuals(condition_model))
dfbeta(condition_model)
library(lme4)
which(is.na(data$time_to_right))
load("~/Year4/Bachelor's project/data/filtered_trials.csv")
base_dataset = read_csv("~/Year4/Bachelor's project/data/filtered_trials.csv")
base_dataset = read_csv("/Year4/Bachelor's project/data/filtered_trials.csv")
base_dataset = read.csv("~/Year4/Bachelor's project/data/filtered_trials.csv")
no_outliers_3 = read.csv("~/Year4/Bachelor's project/data/filtered_trials_outliers_removed_3.csv")
no_outliers_4 = read.csv("~/Year4/Bachelor's project/data/filtered_trials_outliers_removed_4.csv")
no_outliers_5 = read.csv("~/Year4/Bachelor's project/data/filtered_trials_outliers_removed_5.csv")
library(lme4)
experiment.model = lmer(log(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=base_dataset)
experiment.model.nooutliers3 = lmer(log(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=no_outliers_3)
experiment.model.nooutliers4 = lmer(log(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=no_outliers_4)
experiment.model.nooutliers5 = lmer(log(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=no_outliers_5)
summary(experiment.model.nooutliers3)
experiment.model.c = lmer(log(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=base_dataset, REML=FALSE)
experiment.model.nooutliers3.c = lmer(log(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=no_outliers_3, REML=FALSE)
experiment.model.nooutliers4.c = lmer(log(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=no_outliers_4, REML=FALSE)
experiment.model.nooutliers5.c = lmer(log(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=no_outliers_5, REML=FALSE)
experiment.model.null = lmer(log(time_to_right) ~ (1+condition|participant) + (1+condition|set), data=base_dataset, REML=FALSE)
experiment.model.nooutliers3.null = lmer(log(time_to_right) ~ (1+condition|participant) + (1+condition|set), data=no_outliers_3, REML=FALSE)
experiment.model.nooutliers4.null = lmer(log(time_to_right) ~ (1+condition|participant) + (1+condition|set), data=no_outliers_4, REML=FALSE)
experiment.model.nooutliers5.null = lmer(log(time_to_right) ~ (1+condition|participant) + (1+condition|set), data=no_outliers_5, REML=FALSE)
anova(experiment.model.null, experiment.model.c)
anova(experiment.model.nooutliers3.null, experiment.model.noountliers3.c)
anova(experiment.model.nooutliers3.null, experiment.model.nooutliers3.c)
anova(experiment.model.nooutliers4.null, experiment.model.nooutliers4.c)
anova(experiment.model.nooutliers5.null, experiment.model.nooutliers5.c)
summary(experiment.model)
linear.model = lm(log(time_to_right) ~ condition, base_dataset)
plot(fitted(linear.model), residuals(linear.model))
hist(residuals(linear.model))
qqnorm(residuals(linear.model))
plots(experiment.model)
hist(residuals(experiment.model))
qqnorm(residuals(experiment.model))
plot(fitted(experiment.model), residuals(experiment.model))
experiment.model.sqrt = lmer(sqrt(time_to_right) ~ condition + (1+condition|participant) + (1+condition|set), data=base_dataset)
plot(fitted(experiment.model.sqrt), residuals(experiment.model.sqrt))
rm(list = ls())
setwd("C:/Users/Gebruiker/Documents/Masters CCS/Year1/Block12/Methods in CCS/algebra/methodsccs-group3/methodsccs-group3/")
dat = read.csv("fulldata_combined.csv", header = TRUE)
head(fulldata_combined)
head(dat)
library(dplyr)
dat_filtered <- filter(dat, correct = 0)
dat_filtered <- filter(dat, correct == 0)
head(dat_filtered)
dat_filtered <- filter(dat, correct == 1)
head(dat_filtered)
dat_filtered
summary(dat_filtered)
dat_filtered <- select(dat_filtered, -correct)
head(dat_filtered)
dat_filtered <- select(dat_filtered, -practice)
summary(dat_filtered)
table(dat_filtered$subject)
summary(dat_filtered)
table(dat_filtered$problem_type)
table(dat$filtered$Condition)
table(dat_filtered$Condition)
dat_filtered$subject_nr <- as.factor(dat_filtered$subject_nr)
dat_filtered$problem_type <- as.factor(dat_filtered$problem_type)
head(dat_filtered)
library(mgcv)
histogram(dat_filtered$RT_response)
hist(dat_filtered$RT_response)
hist(log(dat_filtered$RT_response))
dat_filtered$log_RT <- log(dat_filtered$RT_response)
head(dat_filtered)
head(dat_filtered)
dat_filtered$cLog = scale(dat_filtered$log_RT)
head(dat_filtered)
dat_filtered$cLog_RT = scale(dat_filtered$log_RT)
#Including fixed effects for problem type and condition and their interactions, since that's what we're interested in investigating
#Including random intercepts for subject bc we're controlling for subject variation
#Including random slopes for subject:problem_type and subject:condition bc we expect them to interact
#Including random intercepts for trial bc it might have an effect that we want to control for
#We will include random slopes for trial and problem_type/condition and see if they contribute anything to the model
m1 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re') + s(trial, condition, bs='re'), data=dat)
dat_filtered$cLog_RT = scale(dat_filtered$log_RT)
#Including fixed effects for problem type and condition and their interactions, since that's what we're interested in investigating
#Including random intercepts for subject bc we're controlling for subject variation
#Including random slopes for subject:problem_type and subject:condition bc we expect them to interact
#Including random intercepts for trial bc it might have an effect that we want to control for
#We will include random slopes for trial and problem_type/condition and see if they contribute anything to the model
m1 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re') + s(trial, condition, bs='re'), data=dat_filtered)
#Including fixed effects for problem type and condition and their interactions, since that's what we're interested in investigating
#Including random intercepts for subject bc we're controlling for subject variation
#Including random slopes for subject:problem_type and subject:condition bc we expect them to interact
#Including random intercepts for trial bc it might have an effect that we want to control for
#We will include random slopes for trial and problem_type/condition and see if they contribute anything to the model
m1 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re') + s(trial, Condition, bs='re'), data=dat_filtered)
dat_filtered$trial <- as.factor(dat_filtered$trial)
m1 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re') + s(trial, Condition, bs='re'), data=dat_filtered)
dat_filtered$subject_nr <- as.factor(dat_filtered$subject_nr)
dat_filtered$problem_type <- as.factor(dat_filtered$problem_type)
dat_filtered$trial <- as.factor(dat_filtered$trial)
dat_filtered$Condition <- as.factor(dat_filtered$Condition)
m1 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re') + s(trial, Condition, bs='re'), data=dat_filtered)
summary(m1)
gam.vcomp(m1)
m1 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re'), data=dat_filtered)
m1 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re') + s(trial, Condition, bs='re'), data=dat_filtered)
m2 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re'), data=dat_filtered)
summary(m2)
gam.vcomp(m2)
compareML(m1, m2)
library(itsadug)
compareML(m1, m2)
#It says model 2 is preferred, which we expected based on the significance scores
#Let's try removing the random slopes for subject
m3 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, problem_type, bs='re') + s(trial, problem_type, bs='re'), data=dat_filtered)
compareML(m2, m3)
#m2 has lower fREML score, so we are sticking with m2
m4 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re'), data=dat_filtered)
compareML(m2, m4)
#Once again, model m2 has lower fREML score, so we're sticking with that (even if it uses more edf)
#We won't even try removing the last random slope (trial:problem type) bc the significance scores already tell us it's very significant
#Our final model is then m2, we have no reason to remove anything else
m5 = bam(cLog_RT ~ Condition + problem_type + Condition:problem_type + s(subject_nr, bs='re') + s(subject_nr, problem_type, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, problem_type, bs='re'), data=dat_filtered)
compareML(m2, m5)
#Indeed, the fREML score is the same but model m5 uses less edf, so we are sticking with that
#There is nothing else worth removing, so our final model is model m5
summary(m5)
plot(m5)
m5_null = bam(cLog_RT ~ problem_type + s(subject_nr, bs='re') + s(subject_nr, problem_type, bs='re') + s(trial, problem_type, bs='re'), data=dat_filtered)
anova(m5, m5_null)
gamtabs(m5, type="HTML")
hist(dat_filtered$log_RT)
hist(dat_filtered$cLog_RT)
dat_onlyhardestdiff <- filter(dat_filtered, problem_type == 5)
m_hardest = bam(cLog_RT ~ Condition + s(subject_nr, bs='re') + s(subject_nr, Condition, bs='re'), data=dat_onlyhardestdiff)
summary(m_hardest)
m_hardest2 = bam(cLog_RT ~ Condition + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, Condition, bs='re'), data=dat_onlyhardestdiff)
summary(m_hardest_2)
summary(m_hardest2)
m_hardest3 = bam(cLog_RT ~ Condition + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, Condition, bs='re'), data=dat_onlyhardestdiff)
summary(m_hardest3)
summary(m_hardest_2)
m_hardest2 = bam(cLog_RT ~ Condition + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, Condition, bs='re'), data=dat_onlyhardestdiff)
summary(m_hardest_2)
m_hardest2 = bam(cLog_RT ~ Condition + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, Condition, bs='re'), data=dat_onlyhardestdiff)
summary(m_hardest2)
m_hardest3 = bam(cLog_RT ~ Condition + s(subject_nr, bs='re') + s(trial, bs='re') + s(subject_nr, Condition, bs='re') + s(trial, Condition, bs='re'), data=dat_onlyhardestdiff)
summary(m_hardest3)
